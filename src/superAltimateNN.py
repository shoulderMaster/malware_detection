#!/usr/bin/python3
from config import *
import tensorflow as tf
import pandas as pd
import os
import sys
from random import shuffle
from shutil import copy2
from math import sqrt


class Model :
    def __init__(self, config, LSTM=None, DNN=None) :

        if DNN == True :
            self._makeMultipleIndependentDNNGraph(config)
            self._makeTensorboardOperator()

    def _makeMultipleIndependentDNNGraph(self, config) :

        num_input=config.inputData.num_input
        num_label=config.inputData.num_label
        n_hidden_1=config.learning.n_hidden_1
        n_hidden_2=config.learning.n_hidden_2
        n_hidden_3=config.learning.n_hidden_3
        learning_rate=config.learning.learning_rate

        tf.reset_default_graph()
        g = tf.Graph()
        g.as_default()

        X = self.X = tf.placeholder(tf.float32, shape=(None, num_input))
        Y = self.Y = tf.placeholder(tf.float32, shape=(None, num_label))
        keep_prob = self.keep_prob = tf.placeholder(tf.float32)

        initializer = self.initializer = tf.contrib.layers.xavier_initializer()

        self.weights = weights = {
            'h1': [tf.Variable(initializer([num_input, n_hidden_1])) for i in range(0,num_label)],
            'h2': [tf.Variable(initializer([n_hidden_1, n_hidden_2])) for i in range(0,num_label)],
            'h3': [tf.Variable(initializer([n_hidden_2, n_hidden_3])) for i in range(0,num_label)],
            'out': [tf.Variable(initializer([n_hidden_3, 1])) for i in range(0,num_label)]
        }


        self.biases = biases = {
            'h1': [tf.Variable(initializer([n_hidden_1,])) for i in range(0,num_label)],
            'h2': [tf.Variable(initializer([n_hidden_2,])) for i in range(0,num_label)],
            'h3': [tf.Variable(initializer([n_hidden_3,])) for i in range(0,num_label)],
            'out': [tf.Variable(initializer([1,])) for i in range(0, num_label)]
        }

        out_layer = []
        for i in range(0, num_label) :

            wx1 =tf.add(tf.matmul(X, weights['h1'][i]), biases['h1'][i])
            layer_1 = tf.nn.sigmoid(wx1)
            layer_1 = tf.nn.dropout(layer_1, keep_prob)

            wx2 = tf.add(tf.matmul(layer_1, weights['h2'][i]), biases['h2'][i])
            layer_2 = wx2
            layer_2 = tf.nn.sigmoid(layer_2)
            layer_2 = tf.nn.dropout(layer_2, keep_prob)

            wx3 = tf.add(tf.matmul(layer_2, weights['h3'][i]), biases['h3'][i])
            layer_3 = wx3
            layer_3 = tf.nn.sigmoid(wx3)
            layer_3 = tf.nn.dropout(layer_3, keep_prob)

            pred = tf.add(tf.matmul(layer_3, weights['out'][i]), biases['out'][i], name="pred")
            out_layer.append(pred)

        Y_pred = self.Y_pred_op = tf.squeeze(tf.stack(out_layer, axis=1), 2)

        self.train_op, self.loss_op, self.saver, self.optimizer, self.pred, self.correct_pred, self.accuracy = self._makeOperator(config, Y_pred, Y)


    def _makeOperator(self, config, Y_pred, Y) :
        learning_rate=config.learning.learning_rate
        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_pred, labels=Y))
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = optimizer.minimize(loss_op, name="train_op")
        saver = tf.train.Saver()
        prediction = tf.nn.softmax(Y_pred)
        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        return train_op, loss_op, saver, optimizer, prediction, correct_pred, accuracy

    def _makeTensorboardOperator(self) :
        self._logging_state()
        self._logging_evaluation()

    def _logging_evaluation(self) :
        loss = tf.summary.scalar("loss", self.loss_op)
        accuracy = tf.summary.scalar("accuracy", self.accuracy)
        self.logging_evaluation = tf.summary.merge([loss, accuracy])

    def _logging_state(self) :
        op_list = []
        for w_key in self.weights.keys() :
            for out_idx, weight in enumerate(self.weights[w_key]) :
                op_name = "out%d_%s_weight" % (out_idx, w_key)
                t_op = tf.summary.histogram(op_name, weight)
                op_list.append(t_op)

        for b_key in self.biases.keys() :
            for out_idx, bias in enumerate(self.biases[b_key]) :
                op_name = "out%d_%s_bias" % (out_idx, b_key)
                t_op = tf.summary.histogram(op_name, bias)
                op_list.append(t_op)

        self.logging_state = tf.summary.merge(op_list)


class InputData :

    def __init__(self, inputDataConfiguation, LSTM=None, DNN=None) :

        if DNN == True :
            self.train_x, self.train_y, self.test_x, self.test_y, self.testSetIndex\
            = self.getInputData(\
                    pathOfTrainSet=inputDataConfiguation.pathOfTrainSet,\
                    pathOfTestSet=inputDataConfiguation.pathOfTestSet,\
                    train_ratio=inputDataConfiguation.train_ratio,\
                    num_input=inputDataConfiguation.num_input,\
                    normalization_method="minmax",\
                    data_randomization=True,\
                    data_uniformalization=False,\
                    RNN=False)


    def _splitXY(self, df, num_input) :
        x_df = df.iloc[:,:num_input]
        y_df = df.iloc[:,num_input:]
        return x_df, y_df

    def _splitTrainTest(self, x_df, y_df, train_ratio) :
        div_num = int(len(x_df.index)*train_ratio)
        x_train_df = x_df.iloc[:div_num,:]
        x_test_df = x_df.iloc[div_num:,:]
        y_train_df = y_df.iloc[:div_num,:]
        y_test_df = y_df.iloc[div_num:,:]
        return x_train_df, x_test_df, y_train_df, y_test_df

    def _minMaxNormalization(self, x_train_df, x_test_df) :
        mean = x_train_df.min()
        std = x_train_df.max() - x_train_df.min() + 0.00001
        x_train_normedDf = (x_train_df-mean)/std
        x_test_normedDf = (x_test_df-mean)/std
        return x_train_normedDf, x_test_normedDf

    def _standardzation(self, x_train_df, x_test_df) :
        mean = x_train_df.mean()
        std = x_train_df.std() + 0.00001
        x_train_normedDf = (x_train_df-mean)/std
        x_test_normedDf = (x_test_df-mean)/std
        return x_train_normedDf, x_test_normedDf

    def _toDNNList(self, x_train_df, y_train_df, x_test_df, y_test_df) :

        testSetIndex = y_test_df.index
        x_train_list = x_train_df.values.tolist()
        y_train_list = y_train_df.values.tolist()
        x_test_list = x_test_df.values.tolist()
        y_test_list = y_test_df.values.tolist()
        return x_train_list, y_train_list, x_test_list, y_test_list, testSetIndex

    def _shuffleList(self, listX, listY) :
        tupleList = [(listX[i], listY[i]) for i in range(0, len(listY))]
        shuffle(tupleList)
        listX = [tupleList[i][0] for i in range(0, len(listY))]
        listY = [tupleList[i][1] for i in range(0, len(listY))]
        return listX, listY

    def getInputData(\
            self,\
            pathOfTrainSet,\
            pathOfTestSet,\
            train_ratio,\
            num_input,\
            num_recurrence=0,\
            normalization_method="minmax",\
            data_randomization=True,\
            data_uniformalization=False,\
            RNN=False) :

        train_df = pd.read_csv(pathOfTrainSet).set_index("file_name").sample(frac=1)
        test_df = pd.read_csv(pathOfTestSet).set_index("file_name").sample(frac=1)

        x_train_df, y_train_df = self._splitXY(train_df, num_input)
        x_test_df, y_test_df = self._splitXY(test_df, num_input)

        if data_uniformalization == True :
            x_df = x_df.rank()

        if normalization_method == "minmax" :
            x_train_df, x_test_df = self._minMaxNormalization(x_train_df, x_test_df)
        elif normalization_method == "standarzation" :
            x_train_df, x_test_df = self._standardzation(x_train_df, x_test_df)

        if RNN == True :
            train_x, train_y, test_x, test_y, testSetIndex = self._toRNNList(x_train_df, y_train_df, x_test_df, y_test_df, num_recurrence)
        else :
            train_x, train_y, test_x, test_y, testSetIndex = self._toDNNList(x_train_df, y_train_df, x_test_df, y_test_df)

        if data_randomization == True :
            train_x, train_y= self._shuffleList(train_x, train_y)

        return train_x, train_y, test_x, test_y, testSetIndex


class NeuralNetwork :

    def __init__(self, LSTM=None, DNN=None) :

        if DNN == True :
            self.config = Configuration(DNN=True)
            self.model = Model(self.config, DNN=True)
            self.inputData = InputData(self.config.inputData, DNN=True)

        self._makeDir()

    def _makeDir(self) :
        if not os.path.exists(self.config.checkPoint.pathOfCheckpoint):
            os.makedirs(self.config.checkPoint.pathOfCheckpoint)

        if not os.path.exists(self.config.checkPoint.pathOfCheckpoint+"/"+sys.argv[0]):
            copy2(sys.argv[0], self.config.checkPoint.pathOfCheckpoint)
            copy2("config.py", self.config.checkPoint.pathOfCheckpoint)

    def doTraining(self) :

        trainX=self.inputData.train_x
        trainY=self.inputData.train_y
        testX=self.inputData.test_x
        testY=self.inputData.test_y
        x_placeholder=self.model.X
        y_placeholder=self.model.Y
        keep_prob = self.model.keep_prob
        train_op=self.model.train_op
        loss_op=self.model.loss_op
        Y_pred_op=self.model.Y_pred_op
        pred, correct_pred, accuracy = self.model.pred, self.model.correct_pred, self.model.accuracy

        saver=self.model.saver
        howManyEpoch=self.config.learning.numLearningEpoch
        display_step=self.config.learning.display_step
        input_keep_prob = output_keep_prob = self.config.learning.input_keep_prob
        save_step=self.config.checkPoint.save_step
        pathOfCheckpoint=self.config.checkPoint.pathOfCheckpoint
        batchDivider=self.config.learning.batchDivider
        filenameOfCheckpoint=self.config.checkPoint.filenameOfCheckpoint

        init_step = 0

        with tf.Session() as sess :

            state_writer = tf.summary.FileWriter(pathOfCheckpoint+"log/state")
            train_writer = tf.summary.FileWriter(pathOfCheckpoint+"log/train")
            test_writer = tf.summary.FileWriter(pathOfCheckpoint+"log/test")

            sess.run(tf.global_variables_initializer())

            #resotre check point
            ckpt_path = tf.train.latest_checkpoint(pathOfCheckpoint)
            if ckpt_path :
                saver.restore(sess, ckpt_path)
                init_step = int(ckpt_path.rsplit("-")[1])

            for step in range(init_step, howManyEpoch) :

                if step % display_step == 0 :
                    loss, cur_lr = sess.run([loss_op, self.model.optimizer._lr_t], feed_dict={x_placeholder: trainX, y_placeholder: trainY, keep_prob: input_keep_prob})
                    acc, testPredict = sess.run([accuracy, pred], feed_dict={x_placeholder: testX, y_placeholder: testY, keep_prob: 1.0})
                    print("\n"+"="*100)
                    print("\nEpoch "+str(step)+", cost = ", loss, ("learningRate : %.6f" % (cur_lr)))
                    self.result = sess.run(pred, feed_dict={x_placeholder: testX, keep_prob: 1.0})
                    result_df = self.getResultAsDf()
                    self._modelEvaluation(result_df=result_df)
                    self._logging_with_tensorboard(sess, state_writer, train_writer, test_writer, step)
                    if step == (howManyEpoch-1) :
                        break

                if step % save_step == 0 :
                    print("save current state")
                    saver.save(sess, pathOfCheckpoint+filenameOfCheckpoint, global_step=step)

                self._batchTrainer(sess=sess,\
                        train_op=train_op,\
                        batch_divider=batchDivider,\
                        trainX=trainX,\
                        trainY=trainY,\
                        x_placeholder=x_placeholder,\
                        y_placeholder=y_placeholder,\
                        keep_prob=keep_prob,\
                        output_keep_prob=output_keep_prob,\
                        input_keep_prob=input_keep_prob)


            self.result = sess.run(pred, feed_dict={x_placeholder: testX, keep_prob: 1.0})
            return self.result

    def _logging_with_tensorboard(self, sess, state_writer, train_writer, test_writer, step) :
        trainX=self.inputData.train_x
        trainY=self.inputData.train_y
        testX=self.inputData.test_x
        testY=self.inputData.test_y

        x_placeholder=self.model.X
        y_placeholder=self.model.Y
        keep_prob = self.model.keep_prob

        input_keep_prob=self.config.learning.input_keep_prob

        state_writer.add_summary(sess.run(self.model.logging_state, feed_dict={x_placeholder: trainX, y_placeholder: trainY, keep_prob: 1}), step)
        train_writer.add_summary(sess.run(self.model.logging_evaluation, feed_dict={x_placeholder: trainX, y_placeholder: trainY, keep_prob: input_keep_prob}), step)
        test_writer.add_summary(sess.run(self.model.logging_evaluation, feed_dict={x_placeholder: testX, y_placeholder: testY, keep_prob: 1}), step)

    def _modelEvaluation(self, result_df) :
        false_positive_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        false_negative_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        ambiguous_df = result_df.loc[(0.4 < result_df["pred_0"]) & (result_df["pred_0"] < 0.6),:]
        false_negative_cnt = 0
        false_positive_cnt = 0
        total_cnt = len(result_df.index)
        for idx in result_df.index :
            if result_df.loc[idx,"label_0"] == 1 : #malicious case
                if result_df.loc[idx,"pred_0"] < result_df.loc[idx,"pred_1"] : #false negative
                    false_negative_df.loc[idx,:] = result_df.loc[idx,:]
                    false_negative_cnt += 1
            else : #benign case
                if result_df.loc[idx,"pred_0"] > result_df.loc[idx,"pred_1"] : #false positive
                    false_positive_df.loc[idx,:] = result_df.loc[idx,:]
                    false_positive_cnt += 1
        false_negative_df = false_negative_df.dropna()
        false_positive_df = false_positive_df.dropna()
        print("\nFalse positive")
        print(false_positive_df)
        print("\nFalse negative")
        print(false_negative_df)
        print("\nambiguous to classify")
        print(ambiguous_df)
        print("""
        false positive : %.3f %%
        false negative : %.3f %%
        accuracy       : %.3f %%
        """ % (false_positive_cnt/total_cnt*100, false_negative_cnt/total_cnt*100, (total_cnt-false_negative_cnt-false_positive_cnt)/total_cnt*100))

    def getResult(self) :
        pass
        testPredict = []
        with tf.Session() as sess :
            testPredict = sess.run(self.model.Y_pred_op, feed_dict={self.model.X: self.inputData.test_x, self.model.keep_prob: 1.0})
        return testPredict

    def getResult(self, inputList) :
        pass
        testPredict = []
        with tf.Session() as sess :
            testPredict = sess.run(self.model.Y_pred_op, feed_dict={self.model.X: inputList, self.model.keep_prob: 1.0})
        return testPredict

    def _batchTrainer(self, sess, train_op, batch_divider, trainX, trainY, x_placeholder, y_placeholder, keep_prob, output_keep_prob, input_keep_prob) :
        batch_size = len(trainX)//batch_divider+1
        x_batch = []
        y_batch = []
        i = 0

        while (i < len(trainX)) :
            x_batch.append(trainX[i])
            y_batch.append(trainY[i])
            if ((i+1) % batch_size == 0 or i == len(trainX) - 1) :
                _ = sess.run(train_op, feed_dict={x_placeholder: x_batch, y_placeholder: y_batch, keep_prob: input_keep_prob})
                x_batch = []
                y_batch = []
            i += 1

    def getResultAsDf(self,\
            result=None,\
            testY=None) :

        if result == None :
            result=self.result
            testY=self.inputData.test_y
            testSetIndex=self.inputData.testSetIndex

        df = pd.DataFrame(index=testSetIndex)
        for i in range(0, len(result[0])) :
            df["pred_"+str(i)] = [entry[i] for entry in result]
        for i in range(0, len(testY[0])) :
            df["label_"+str(i)] = [entry[i] for entry in testY]
        return df

def main() :
    DNN = NeuralNetwork(DNN=True)
    DNN.doTraining()
    DNN.getResultAsDf().to_csv(DNN.config.learning.resultPath)

main()
