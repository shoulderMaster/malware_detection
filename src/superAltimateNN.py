#!/usr/bin/python3
from config import *
import tensorflow as tf
import pandas as pd
import os
import sys
from random import shuffle
from shutil import copy2
from math import sqrt


class Model :
    def __init__(self,NN, LSTM=None, DNN=None) :

        if DNN == True :
            self.train_op, self.loss_op, self.Y_pred_op, self.saver, self.X, self.Y, self.keep_prob, self.optimizer, self.pred, self.correct_pred, self.accuracy \
            = NN._makeMultipleIndependentDNNGraph()

class InputData :

    def __init__(self, inputDataConfiguation, LSTM=None, DNN=None) :

        if DNN == True :
            self.train_x, self.train_y, self.test_x, self.test_y, self.testSetIndex\
            = self.getInputData(\
                    pathOfDNNinputData=inputDataConfiguation.pathOfinputData,\
                    train_ratio=inputDataConfiguation.train_ratio,\
                    num_input=inputDataConfiguation.num_input,\
                    normalization_method="minmax",\
                    data_randomization=True,\
                    data_uniformalization=False,\
                    RNN=False)

        self.originDf = pd.read_csv("DNN_input_data.csv").set_index("file_name")


    def _splitXY(self, df, num_input) :
        x_df = df.iloc[:,:num_input]
        y_df = df.iloc[:,num_input:]
        return x_df, y_df

    def _splitTrainTest(self, x_df, y_df, train_ratio) :
        div_num = int(len(x_df.index)*train_ratio)
        x_train_df = x_df.iloc[:div_num,:]
        x_test_df = x_df.iloc[div_num:,:]
        y_train_df = y_df.iloc[:div_num,:]
        y_test_df = y_df.iloc[div_num:,:]
        return x_train_df, x_test_df, y_train_df, y_test_df

    def _minMaxNormalization(self, x_train_df, x_test_df) :
        mean = x_train_df.min()
        std = x_train_df.max() - x_train_df.min() + 0.00001
        x_train_normedDf = (x_train_df-mean)/std
        x_test_normedDf = (x_test_df-mean)/std
        return x_train_normedDf, x_test_normedDf

    def _standardzation(self, x_train_df, x_test_df) :
        mean = x_train_df.mean()
        std = x_train_df.std() + 0.00001
        x_train_normedDf = (x_train_df-mean)/std
        x_test_normedDf = (x_test_df-mean)/std
        return x_train_normedDf, x_test_normedDf

    def _toDNNList(self, x_train_df, y_train_df, x_test_df, y_test_df) :

        testSetIndex = y_test_df.index
        x_train_list = x_train_df.values.tolist()
        y_train_list = y_train_df.values.tolist()
        x_test_list = x_test_df.values.tolist()
        y_test_list = y_test_df.values.tolist()
        return x_train_list, y_train_list, x_test_list, y_test_list, testSetIndex

    def _shuffleList(self, listX, listY) :
        tupleList = [(listX[i], listY[i]) for i in range(0, len(listY))]
        shuffle(tupleList)
        listX = [tupleList[i][0] for i in range(0, len(listY))]
        listY = [tupleList[i][1] for i in range(0, len(listY))]
        return listX, listY

    def getInputData(\
            self,\
            pathOfDNNinputData,\
            train_ratio,\
            num_input,\
            num_recurrence=0,\
            normalization_method="minmax",\
            data_randomization=True,\
            data_uniformalization=False,\
            RNN=False) :

        data_df = pd.read_csv(pathOfDNNinputData).set_index("file_name").sample(frac=1)

        x_df, y_df = self._splitXY(data_df, num_input)

        if data_uniformalization == True :
            x_df = x_df.rank()

        x_train_df, x_test_df, y_train_df, y_test_df = self._splitTrainTest(x_df, y_df, train_ratio)

        if normalization_method == "minmax" :
            x_train_df, x_test_df = self._minMaxNormalization(x_train_df, x_test_df)
        elif normalization_method == "standarzation" :
            x_train_df, x_test_df = self._standardzation(x_train_df, x_test_df)

        if RNN == True :
            train_x, train_y, test_x, test_y, testSetIndex = self._toRNNList(x_train_df, y_train_df, x_test_df, y_test_df, num_recurrence)
        else :
            train_x, train_y, test_x, test_y, testSetIndex = self._toDNNList(x_train_df, y_train_df, x_test_df, y_test_df)

        if data_randomization == True :
            train_x, train_y= self._shuffleList(train_x, train_y)

        return train_x, train_y, test_x, test_y, testSetIndex


class NeuralNetwork :

    def __init__(self, LSTM=None, DNN=None) :

        if DNN == True :
            self.config = Configuration(DNN=True)
            self.model = Model(self, DNN=True)
            self.inputData = InputData(self.config.inputData, DNN=True)

        self.result = []
        if not os.path.exists(self.config.checkPoint.pathOfCheckpoint):
            os.makedirs(self.config.checkPoint.pathOfCheckpoint)

        if not os.path.exists(self.config.checkPoint.pathOfCheckpoint+"/"+sys.argv[0]):
            copy2(sys.argv[0], self.config.checkPoint.pathOfCheckpoint)



    def _makeMultipleIndependentDNNGraph(\
            self,\
            num_input=None,\
            num_label=None,\
            n_hidden_1=None,\
            n_hidden_2=None,\
            n_hidden_3=None,\
            learning_rate=None,\
            ) :

        if num_input == None :
            num_input=self.config.inputData.num_input
            num_label=self.config.inputData.num_label
            n_hidden_1=self.config.learning.n_hidden_1
            n_hidden_2=self.config.learning.n_hidden_2
            n_hidden_3=self.config.learning.n_hidden_3
            learning_rate=self.config.learning.learning_rate

        tf.reset_default_graph()
        g = tf.Graph()
        g.as_default()

        X = tf.placeholder(tf.float32, shape=(None, num_input))
        Y = tf.placeholder(tf.float32, shape=(None, num_label))
        keep_prob = tf.placeholder(tf.float32)

        initializer = tf.contrib.layers.xavier_initializer()

        weights = {
            'h1': [tf.Variable(initializer([num_input, n_hidden_1])) for i in range(0,num_label)],
            'h2': [tf.Variable(initializer([n_hidden_1, n_hidden_2])) for i in range(0,num_label)],
            'h3': [tf.Variable(initializer([n_hidden_2, n_hidden_3])) for i in range(0,num_label)],
            'out': [tf.Variable(initializer([n_hidden_3, 1])) for i in range(0,num_label)]
        }


        biases = {
            'b1': [tf.Variable(initializer([n_hidden_1,])) for i in range(0,num_label)],
            'b2': [tf.Variable(initializer([n_hidden_2,])) for i in range(0,num_label)],
            'b3': [tf.Variable(initializer([n_hidden_3,])) for i in range(0,num_label)],
            'out': [tf.Variable(initializer([1,])) for i in range(0, num_label)]
        }

        out_layer = []
        for i in range(0, num_label) :

            wx1 =tf.add(tf.matmul(X, weights['h1'][i]), biases['b1'][i])
            layer_1 = tf.nn.sigmoid(wx1)
            layer_1 = tf.nn.dropout(layer_1, keep_prob)

            wx2 = tf.add(tf.matmul(layer_1, weights['h2'][i]), biases['b2'][i])
            layer_2 = wx2
            layer_2 = tf.nn.sigmoid(layer_2)
            layer_2 = tf.nn.dropout(layer_2, keep_prob)

            wx3 = tf.add(tf.matmul(layer_2, weights['h3'][i]), biases['b3'][i])
            layer_3 = wx3
            layer_3 = tf.nn.sigmoid(wx3)
            layer_3 = tf.nn.dropout(layer_3, keep_prob)

            pred = tf.add(tf.matmul(layer_3, weights['out'][i]), biases['out'][i], name="pred")
            out_layer.append(pred)

        Y_pred = tf.squeeze(tf.stack(out_layer, axis=1), 2)
        #Y_pred = tf.nn.sigmoid(Y_pred)
        train_op, loss_op, saver, optimizer , pred, correct_pred, accuracy = self._makeTrainOperator(Y_pred, Y)

        return train_op, loss_op, Y_pred, saver, X, Y, keep_prob, optimizer, pred, correct_pred, accuracy


    def _makeTrainOperator(self, Y_pred, Y) :
        learning_rate=self.config.learning.learning_rate
        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_pred, labels=Y))
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = optimizer.minimize(loss_op, name="train_op")
        saver = tf.train.Saver()
        prediction = tf.nn.softmax(Y_pred)
        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        return train_op, loss_op, saver, optimizer, prediction, correct_pred, accuracy

    def doTraining(\
            self,\
            trainX=None,\
            trainY=None,\
            testX=None,\
            testY=None,\
            x_placeholder=None,\
            y_placeholder=None,\
            keep_prob=None,\
            train_op=None,\
            loss_op=None,\
            Y_pred_op=None,\
            saver=None,\
            howManyEpoch=None,\
            display_step=None,\
            output_keep_prob=None,\
            input_keep_prob=None,\
            save_step=None,\
            pathOfCheckpoint=None,\
            batchDivider=None,\
            filenameOfCheckpoint=None) :

        if trainX == None :
            trainX=self.inputData.train_x
            trainY=self.inputData.train_y
            testX=self.inputData.test_x
            testY=self.inputData.test_y
            x_placeholder=self.model.X
            y_placeholder=self.model.Y
            keep_prob=self.model.keep_prob
            train_op=self.model.train_op
            loss_op=self.model.loss_op
            Y_pred_op=self.model.Y_pred_op
            pred, correct_pred, accuracy = self.model.pred, self.model.correct_pred, self.model.accuracy

            saver=self.model.saver
            howManyEpoch=self.config.learning.numLearningEpoch
            display_step=self.config.learning.display_step
            input_keep_prob=self.config.learning.input_keep_prob
            save_step=self.config.checkPoint.save_step
            pathOfCheckpoint=self.config.checkPoint.pathOfCheckpoint
            batchDivider=self.config.learning.batchDivider
            filenameOfCheckpoint=self.config.checkPoint.filenameOfCheckpoint

        init_step = 0

        with tf.Session() as sess :

            sess.run(tf.global_variables_initializer())

            #resotre check point
            ckpt_path = tf.train.latest_checkpoint(pathOfCheckpoint)
            if ckpt_path :
                saver.restore(sess, ckpt_path)
                init_step = int(ckpt_path.rsplit("-")[1])

            learningRateModificationCount = 0
            for step in range(init_step, howManyEpoch) :

                if step % display_step == 0 :
                    loss, cur_lr = sess.run([loss_op, self.model.optimizer._lr_t], feed_dict={x_placeholder: trainX, y_placeholder: trainY, keep_prob: input_keep_prob})
                    acc, testPredict = sess.run([accuracy, pred], feed_dict={x_placeholder: testX, y_placeholder: testY, keep_prob: 1.0})
                    print("\n"+"="*100)
                    print("\nEpoch "+str(step)+", cost = ", loss, ("learningRate : %.6f" % (cur_lr)))
                    self.result = sess.run(pred, feed_dict={x_placeholder: testX, keep_prob: 1.0})
                    result_df = self.getResultAsDf()
                    self._modelEvaluation(result_df=result_df)
                    if step == (howManyEpoch-1) :
                        break

                if step % save_step == 0 :
                    print("save current state")
                    saver.save(sess, pathOfCheckpoint+filenameOfCheckpoint, global_step=step)

                self._batchTrainer(sess=sess,\
                        train_op=train_op,\
                        batch_divider=batchDivider,\
                        trainX=trainX,\
                        trainY=trainY,\
                        x_placeholder=x_placeholder,\
                        y_placeholder=y_placeholder,\
                        keep_prob=keep_prob,\
                        output_keep_prob=output_keep_prob,\
                        input_keep_prob=input_keep_prob)


            self.result = sess.run(pred, feed_dict={x_placeholder: testX, keep_prob: 1.0})
            return self.result

    def _modelEvaluation(self, result_df) :
        false_positive_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        false_negative_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        ambiguous_df = result_df.loc[(0.4 < result_df["pred_0"]) & (result_df["pred_0"] < 0.6),:]
        false_negative_cnt = 0
        false_positive_cnt = 0
        total_cnt = len(result_df.index)
        for idx in result_df.index :
            if result_df.loc[idx,"label_0"] == 1 : #malicious case
                if result_df.loc[idx,"pred_0"] < result_df.loc[idx,"pred_1"] : #false negative
                    false_negative_df.loc[idx,:] = result_df.loc[idx,:]
                    false_negative_cnt += 1
            else : #benign case
                if result_df.loc[idx,"pred_0"] > result_df.loc[idx,"pred_1"] : #false positive
                    false_positive_df.loc[idx,:] = result_df.loc[idx,:]
                    false_positive_cnt += 1
        false_negative_df = false_negative_df.dropna()
        false_positive_df = false_positive_df.dropna()
        print("\nFalse positive")
        print(false_positive_df)
        print("\nFalse negative")
        print(false_negative_df)
        print("\nambiguous to classify")
        print(ambiguous_df)
        print("""
        false positive : %.3f %%
        false negative : %.3f %%
        accuracy       : %.3f %%
        """ % (false_positive_cnt/total_cnt*100, false_negative_cnt/total_cnt*100, (total_cnt-false_negative_cnt-false_positive_cnt)/total_cnt*100))

    def getResult(self) :
        pass
        testPredict = []
        with tf.Session() as sess :
            testPredict = sess.run(self.model.Y_pred_op, feed_dict={self.model.X: self.inputData.test_x, self.model.keep_prob: 1.0})
        return testPredict

    def getResult(self, inputList) :
        pass
        testPredict = []
        with tf.Session() as sess :
            testPredict = sess.run(self.model.Y_pred_op, feed_dict={self.model.X: inputList, self.model.keep_prob: 1.0})
        return testPredict

    def _batchTrainer(self, sess, train_op, batch_divider, trainX, trainY, x_placeholder, y_placeholder, keep_prob, output_keep_prob, input_keep_prob) :
        batch_size = len(trainX)//batch_divider+1
        x_batch = []
        y_batch = []
        i = 0

        while (i < len(trainX)) :
            x_batch.append(trainX[i])
            y_batch.append(trainY[i])
            if ((i+1) % batch_size == 0 or i == len(trainX) - 1) :
                _ = sess.run(train_op, feed_dict={x_placeholder: x_batch, y_placeholder: y_batch, keep_prob: input_keep_prob})
                x_batch = []
                y_batch = []
            i += 1

    def getResultAsDf(self,\
            result=None,\
            testY=None) :

        if result == None :
            result=self.result
            testY=self.inputData.test_y
            testSetIndex=self.inputData.testSetIndex

        df = pd.DataFrame(index=testSetIndex)
        for i in range(0, len(result[0])) :
            df["pred_"+str(i)] = [entry[i] for entry in result]
        for i in range(0, len(testY[0])) :
            df["label_"+str(i)] = [entry[i] for entry in testY]
        return df

def main() :
    DNN = NeuralNetwork(DNN=True)
    DNN.doTraining()
    DNN.getResultAsDf().to_csv(DNN.config.learning.resultPath)

main()
