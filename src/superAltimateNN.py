#!/usr/bin/python3
from config import *
import tensorflow as tf
import pandas as pd
import os
import sys
from random import shuffle
from shutil import copy2
from math import sqrt
from matplotlib import pyplot as plt
import io


class Model :
    def __init__(self, config, model) :

        self.model = model
        if model == "DNN":
            self._makeMultipleIndependentDNNGraph(config)
        elif model == "support_vector_machine" :
            self._makeSupportVectorMachine(config)
        elif model == "linear_neural_network" :
            self._makeLinearNeuralNetwork(config)

        self._makeTensorboardOperator()

    def _makeMultipleIndependentDNNGraph(self, config) :

        num_input=config.inputData.num_input
        num_label=config.inputData.num_label
        n_hidden_1=config.learning.n_hidden_1
        n_hidden_2=config.learning.n_hidden_2
        n_hidden_3=config.learning.n_hidden_3
        learning_rate=config.learning.learning_rate

        tf.reset_default_graph()
        g = tf.Graph()
        g.as_default()

        X = self.X = tf.placeholder(tf.float32, shape=(None, num_input))
        Y = self.Y = tf.placeholder(tf.float32, shape=(None, num_label))
        keep_prob = self.keep_prob = tf.placeholder(tf.float32)

        initializer = self.initializer = tf.contrib.layers.xavier_initializer()

        self.weights = weights = {
            'h1': [tf.Variable(initializer([num_input, n_hidden_1])) for i in range(0,num_label)],
            'h2': [tf.Variable(initializer([n_hidden_1, n_hidden_2])) for i in range(0,num_label)],
            'h3': [tf.Variable(initializer([n_hidden_2, n_hidden_3])) for i in range(0,num_label)],
            'h4': [tf.Variable(initializer([n_hidden_2, n_hidden_3])) for i in range(0,num_label)],
            'out': [tf.Variable(initializer([n_hidden_3, 1])) for i in range(0,num_label)]
        }


        self.biases = biases = {
            'h1': [tf.Variable(initializer([n_hidden_1,])) for i in range(0,num_label)],
            'h2': [tf.Variable(initializer([n_hidden_2,])) for i in range(0,num_label)],
            'h3': [tf.Variable(initializer([n_hidden_3,])) for i in range(0,num_label)],
            'h4': [tf.Variable(initializer([n_hidden_3,])) for i in range(0,num_label)],
            'out': [tf.Variable(initializer([1,])) for i in range(0, num_label)]
        }

        out_layer = []
        for i in range(0, num_label) :

            wx1 =tf.add(tf.matmul(X, weights['h1'][i]), biases['h1'][i])
            layer_1 = tf.nn.sigmoid(wx1)
            layer_1 = tf.nn.dropout(layer_1, keep_prob)

            wx2 = tf.add(tf.matmul(layer_1, weights['h2'][i]), biases['h2'][i])
            layer_2 = wx2
            layer_2 = tf.nn.sigmoid(layer_2)
            layer_2 = tf.nn.dropout(layer_2, keep_prob)

            wx3 = tf.add(tf.matmul(layer_2, weights['h3'][i]), biases['h3'][i])
            layer_3 = wx3
            layer_3 = tf.nn.sigmoid(wx3)
            layer_3 = tf.nn.dropout(layer_3, keep_prob)

            wx4 = tf.add(tf.matmul(layer_3, weights['h3'][i]), biases['h3'][i])
            layer_4 = wx4
            layer_4 = tf.nn.sigmoid(wx4)
            layer_4 = tf.nn.dropout(layer_4, keep_prob)

            pred = tf.add(tf.matmul(layer_4, weights['out'][i]), biases['out'][i], name="pred")
            out_layer.append(pred)

        Y_pred = self.Y_pred_op = tf.squeeze(tf.stack(out_layer, axis=1), 2)

        self.train_op, self.loss_op, self.saver, self.optimizer, self.pred, self.correct_pred, self.accuracy = self._makeNNOperator(config, Y_pred, Y)

    def _makeSupportVectorMachine(self, config) :

        num_input=config.inputData.num_input
        num_label=config.inputData.num_label
        learning_rate=config.learning.learning_rate

        tf.reset_default_graph()
        g = tf.Graph()
        g.as_default()

        X = self.X = tf.placeholder(tf.float32, shape=(None, num_input))
        Y = self.Y = tf.placeholder(tf.float32, shape=(None, num_label))
        keep_prob = self.keep_prob = tf.placeholder(tf.float32) #unused

        initializer = self.initializer = tf.random_uniform_initializer(-1, 1)

        self.weights = weights = {
            'out': [tf.Variable(initializer([num_input, 1])) for i in range(0,num_label)]
        }


        self.biases = biases = {
            'out': [tf.Variable(initializer([1,])) for i in range(0, num_label)]
        }

        pred = tf.add(tf.matmul(X, weights['out'][0]), biases['out'][0], name="pred")

        Y_pred = self.Y_pred_op = pred

        self.train_op, self.loss_op, self.saver, self.optimizer, self.pred, self.correct_pred, self.accuracy = self._makeSVMOperator(config, Y_pred, Y)

    def _makeLinearNeuralNetwork(self, config) :

        num_input=config.inputData.num_input
        num_label=config.inputData.num_label
        learning_rate=config.learning.learning_rate

        tf.reset_default_graph()
        g = tf.Graph()
        g.as_default()

        X = self.X = tf.placeholder(tf.float32, shape=(None, num_input))
        Y = self.Y = tf.placeholder(tf.float32, shape=(None, num_label))
        keep_prob = self.keep_prob = tf.placeholder(tf.float32) #unused

        initializer = self.initializer = tf.random_uniform_initializer(-1, 1)

        self.weights = weights = {
            'out': [tf.Variable(initializer([num_input, 1])) for i in range(0, num_label)]
        }


        self.biases = biases = {
            'out': [tf.Variable(initializer([1,])) for i in range(0, num_label)]
        }

        out_layer = []
        for i in range(0, num_label) :
            pred = tf.add(tf.matmul(X, weights['out'][i]), biases['out'][i], name="pred")
            out_layer.append(pred)

        Y_pred = self.Y_pred_op = tf.squeeze(tf.stack(out_layer, axis=1), 2)

        self.train_op, self.loss_op, self.saver, self.optimizer, self.pred, self.correct_pred, self.accuracy = self._makeNNOperator(config, Y_pred, Y)


    def _makeNNOperator(self, config, Y_pred, Y) :
        learning_rate=config.learning.learning_rate
        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_pred, labels=Y))
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = optimizer.minimize(loss_op, name="train_op")
        saver = tf.train.Saver()
        prediction = tf.nn.softmax(Y_pred)
        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        return train_op, loss_op, saver, optimizer, prediction, correct_pred, accuracy

    def _makeSVMOperator(self, config, Y_pred, Y) :
        learning_rate=config.learning.learning_rate
        l2_norm = tf.reduce_mean(tf.square(self.weights["out"][0]))
        alpha = tf.constant([0.1])
        classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1., tf.multiply(Y_pred, Y))))
        loss_op = tf.reshape(tf.add(classification_term, tf.multiply(alpha, l2_norm)), [])
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = optimizer.minimize(loss_op, name="train_op")
        saver = tf.train.Saver()
        prediction = tf.sign(Y_pred)
        correct_pred = tf.equal(prediction, Y)
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        return train_op, loss_op, saver, optimizer, prediction, correct_pred, accuracy

    def _makeTensorboardOperator(self) :
        self._logging_state()
        self._logging_evaluation()
        self._logging_state_image()

    def _logging_evaluation(self) :
        loss = tf.summary.scalar("loss", self.loss_op)
        accuracy = tf.summary.scalar("accuracy", self.accuracy)
        self.logging_evaluation = tf.summary.merge([loss, accuracy])

    def _logging_state(self) :
        op_list = []
        for w_key in self.weights.keys() :
            for out_idx, weight in enumerate(self.weights[w_key]) :
                op_name = "out%d_%s_weight" % (out_idx, w_key)
                t_op = tf.summary.histogram(op_name, weight)
                op_list.append(t_op)

        for b_key in self.biases.keys() :
            for out_idx, bias in enumerate(self.biases[b_key]) :
                op_name = "out%d_%s_bias" % (out_idx, b_key)
                t_op = tf.summary.histogram(op_name, bias)
                op_list.append(t_op)

        self.logging_state = tf.summary.merge(op_list)

    def _logging_state_image(self) :
        self.state_dict = dict()
        self.logging_plot_op_dict = dict()
        self.image_placeholder = tf.placeholder(dtype=tf.string)

        for w_key in self.weights.keys() :
            for out_idx, weight in enumerate(self.weights[w_key]) :
                op_name = "out%d_%s_weight_plot" % (out_idx, w_key)
                self.state_dict[op_name] = tf.reshape(weight, [-1])
                self.state_dict[op_name+"_abs"] = tf.abs(tf.reshape(weight, [-1]))

        for b_key in self.biases.keys() :
            for out_idx, bias in enumerate(self.biases[b_key]) :
                op_name = "out%d_%s_bias_plot" % (out_idx, b_key)
                self.state_dict[op_name] = tf.reshape(bias, [-1])
                self.state_dict[op_name+"_abs"] = tf.abs(tf.reshape(bias, [-1]))

        for op_name in self.state_dict.keys() :
            image = tf.image.decode_png(self.image_placeholder, channels=4)
            image = tf.expand_dims(image, 0)
            self.logging_plot_op_dict[op_name] = tf.summary.merge([tf.summary.image(op_name, image)])

    def _gen_plot(self, data, title="no title") :
        fig = plt.figure()
        plt.plot(data)
        plt.title(title)
        plt.grid()
        buf = io.BytesIO()
        plt.savefig(buf, format="png")
        buf.seek(0)
        plt.close(fig)
        return buf.getvalue()


    def logging_state_to_image(self, sess, writer, step) :
        if self.model == "DNN" :
            return

        for op_name in self.state_dict.keys() :
            state_tensor = self.state_dict[op_name]
            plot_image = self._gen_plot(sess.run(state_tensor).tolist(), op_name)
            writer.add_summary(sess.run(self.logging_plot_op_dict[op_name], feed_dict={self.image_placeholder : plot_image}), step)


class InputData :

    def __init__(self, inputDataConfiguation, model) :
        self.config = inputDataConfiguation
        self.loadInputData(\
                normalization_method="minmax",\
                data_randomization=True,\
                model=model)

    def _splitXY(self, df, num_input, num_label) :
        x_df = df.iloc[:,:num_input]
        y_df = df.iloc[:,num_input:num_input+num_label]
        return x_df, y_df

    def _splitTrainTest(self, x_df, y_df, train_ratio) :
        div_num = int(len(x_df.index)*train_ratio)
        x_train_df = x_df.iloc[:div_num,:]
        x_test_df = x_df.iloc[div_num:,:]
        y_train_df = y_df.iloc[:div_num,:]
        y_test_df = y_df.iloc[div_num:,:]
        return x_train_df, x_test_df, y_train_df, y_test_df

    def _minMaxNormalization(self, x_train_df, x_test_df) :
        mean = x_train_df.min()
        std = x_train_df.max() - x_train_df.min() + 0.00001
        x_train_normedDf = (x_train_df-mean)/std
        x_test_normedDf = (x_test_df-mean)/std
        return x_train_normedDf, x_test_normedDf

    def _standardization(self, x_train_df, x_test_df) :
        mean = x_train_df.mean()
        std = x_train_df.std() + 0.00001
        x_train_normedDf = (x_train_df-mean)/std
        x_test_normedDf = (x_test_df-mean)/std
        return x_train_normedDf, x_test_normedDf

    def _toDNNList(self, x_train_df, y_train_df, x_test_df, y_test_df) :

        testSetIndex = y_test_df.index
        x_train_list = x_train_df.values.tolist()
        y_train_list = y_train_df.values.tolist()
        x_test_list = x_test_df.values.tolist()
        y_test_list = y_test_df.values.tolist()
        return x_train_list, y_train_list, x_test_list, y_test_list, testSetIndex

    def _shuffleList(self, listX, listY) :
        tupleList = [(listX[i], listY[i]) for i in range(0, len(listY))]
        shuffle(tupleList)
        listX = [tupleList[i][0] for i in range(0, len(listY))]
        listY = [tupleList[i][1] for i in range(0, len(listY))]
        return listX, listY

    def loadInputData(\
            self,\
            model,\
            num_recurrence=0,\
            normalization_method="minmax",\
            data_randomization=True) :

        pathOfTrainSet=self.config.pathOfTrainSet
        pathOfTestSet=self.config.pathOfTestSet
        train_ratio=self.config.train_ratio
        num_input=self.config.num_input
        num_label=self.config.num_label

        if "recurrence" in model :
            num_recurrence = self.config.num_recurrence

        train_df = pd.read_csv(pathOfTrainSet).set_index("file_name").sample(frac=1)
        test_df = pd.read_csv(pathOfTestSet).set_index("file_name").sample(frac=1)

        if model == "support_vector_machine" :
            train_df["label_malware"] = train_df["label_malware"].replace(0, -1)
            test_df["label_malware"] = test_df["label_malware"].replace(0,-1)

        x_train_df, y_train_df = self._splitXY(train_df, num_input, num_label)
        x_test_df, y_test_df = self._splitXY(test_df, num_input, num_label)

        if normalization_method == "minmax" :
            x_train_df, x_test_df = self._minMaxNormalization(x_train_df, x_test_df)
        elif normalization_method == "standardization" :
            x_train_df, x_test_df = self._standardization(x_train_df, x_test_df)

        if "recurrence" in model :
            train_x, train_y, test_x, test_y, testSetIndex = self._toRNNList(x_train_df, y_train_df, x_test_df, y_test_df, num_recurrence)
        else :
            train_x, train_y, test_x, test_y, testSetIndex = self._toDNNList(x_train_df, y_train_df, x_test_df, y_test_df)

        if data_randomization == True :
            train_x, train_y= self._shuffleList(train_x, train_y)

        self.train_x, self.train_y, self.test_x, self.test_y, self.testSetIndex = train_x, train_y, test_x, test_y, testSetIndex


class NeuralNetwork :

    def __init__(self, model) :

        if model == "DNN" :
            self.config = Configuration(model=model)
            self.model = Model(self.config, model=model)
            self.inputData = InputData(self.config.inputData, model=model)
        elif model == "support_vector_machine" :
            self.config = Configuration(model=model)
            self.model = Model(self.config, model=model)
            self.inputData = InputData(self.config.inputData, model=model)
        elif model == "linear_neural_network" :
            self.config = Configuration(model=model)
            self.model = Model(self.config, model=model)
            self.inputData = InputData(self.config.inputData, model=model)

        self._makeDir()

    def _makeDir(self) :
        if not os.path.exists(self.config.checkPoint.pathOfCheckpoint):
            os.makedirs(self.config.checkPoint.pathOfCheckpoint)

        if not os.path.exists(self.config.checkPoint.pathOfCheckpoint+"/"+sys.argv[0]):
            copy2(sys.argv[0], self.config.checkPoint.pathOfCheckpoint)
            copy2("config.py", self.config.checkPoint.pathOfCheckpoint)

    def doTraining(self) :

        trainX=self.inputData.train_x
        trainY=self.inputData.train_y
        testX=self.inputData.test_x
        testY=self.inputData.test_y
        x_placeholder=self.model.X
        y_placeholder=self.model.Y
        keep_prob = self.model.keep_prob
        train_op=self.model.train_op
        loss_op=self.model.loss_op
        Y_pred_op=self.model.Y_pred_op
        pred, correct_pred, accuracy = self.model.pred, self.model.correct_pred, self.model.accuracy

        saver=self.model.saver
        howManyEpoch=self.config.learning.numLearningEpoch
        display_step=self.config.learning.display_step
        input_keep_prob = output_keep_prob = self.config.learning.input_keep_prob
        save_step=self.config.checkPoint.save_step
        pathOfCheckpoint=self.config.checkPoint.pathOfCheckpoint
        batchDivider=self.config.learning.batchDivider
        filenameOfCheckpoint=self.config.checkPoint.filenameOfCheckpoint

        init_step = 0

        with tf.Session() as sess :

            state_writer = tf.summary.FileWriter(pathOfCheckpoint+"log/state")
            train_writer = tf.summary.FileWriter(pathOfCheckpoint+"log/train")
            test_writer = tf.summary.FileWriter(pathOfCheckpoint+"log/test")

            sess.run(tf.global_variables_initializer())

            #resotre check point
            ckpt_path = tf.train.latest_checkpoint(pathOfCheckpoint)
            if ckpt_path :
                saver.restore(sess, ckpt_path)
                init_step = int(ckpt_path.rsplit("-")[1])

            for step in range(init_step, howManyEpoch) :

                if step % display_step == 0 :
                    loss, cur_lr = sess.run([loss_op, self.model.optimizer._lr_t], feed_dict={x_placeholder: trainX, y_placeholder: trainY, keep_prob: input_keep_prob})
                    print(loss)
                    acc, testPredict = sess.run([accuracy, pred], feed_dict={x_placeholder: testX, y_placeholder: testY, keep_prob: 1.0})
                    print("\n"+"="*100)
                    print("\nEpoch "+str(step)+", cost = ", loss, ("learningRate : %.6f" % (cur_lr)))
                    self.result = sess.run(pred, feed_dict={x_placeholder: testX, keep_prob: 1.0})
                    result_df = self.getResultAsDf()
                    if self.model.model == "support_vector_machine" :
                        self._modelEvaluationWithSVM(result_df=result_df)
                    else :
                        self._modelEvaluationWithNN(result_df=result_df)
                    self._logging_with_tensorboard(sess, state_writer, train_writer, test_writer, step)
                    if step == (howManyEpoch-1) :
                        break

                if step % save_step == 0 :
                    print("save current state")
                    saver.save(sess, pathOfCheckpoint+filenameOfCheckpoint, global_step=step)

                self._batchTrainer(sess=sess,\
                        train_op=train_op,\
                        batch_divider=batchDivider,\
                        trainX=trainX,\
                        trainY=trainY,\
                        x_placeholder=x_placeholder,\
                        y_placeholder=y_placeholder,\
                        keep_prob=keep_prob,\
                        output_keep_prob=output_keep_prob,\
                        input_keep_prob=input_keep_prob)


            self.result = sess.run(pred, feed_dict={x_placeholder: testX, keep_prob: 1.0})
            return self.result

    def _logging_with_tensorboard(self, sess, state_writer, train_writer, test_writer, step) :
        trainX=self.inputData.train_x
        trainY=self.inputData.train_y
        testX=self.inputData.test_x
        testY=self.inputData.test_y

        x_placeholder=self.model.X
        y_placeholder=self.model.Y
        keep_prob = self.model.keep_prob

        input_keep_prob=self.config.learning.input_keep_prob

        state_writer.add_summary(sess.run(self.model.logging_state, feed_dict={x_placeholder: trainX, y_placeholder: trainY, keep_prob: 1}), step)
        train_writer.add_summary(sess.run(self.model.logging_evaluation, feed_dict={x_placeholder: trainX, y_placeholder: trainY, keep_prob: input_keep_prob}), step)
        test_writer.add_summary(sess.run(self.model.logging_evaluation, feed_dict={x_placeholder: testX, y_placeholder: testY, keep_prob: 1}), step)
        self.model.logging_state_to_image(sess, state_writer, step)

    def _modelEvaluationWithNN(self, result_df) :
        false_positive_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        false_negative_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        ambiguous_df = result_df.loc[(0.4 < result_df["pred_0"]) & (result_df["pred_0"] < 0.6),:]
        false_negative_cnt = 0
        false_positive_cnt = 0
        total_cnt = len(result_df.index)
        tp_df =result_df[(result_df["label_0"] == 1) & (result_df["pred_0"] == 1)]
        fn_df =result_df[(result_df["label_0"] == 1) & (result_df["pred_0"] == 0)]
        tn_df =result_df[(result_df["label_0"] == 0) & (result_df["pred_0"] == 0)]
        fp_df =result_df[(result_df["label_0"] == 0) & (result_df["pred_0"] == 1)]
        tp_cnt = len(tp_df)
        fn_cnt = len(fn_df)
        tn_cnt = len(tn_df)
        fp_cnt = len(fp_df)
        print("\nFalse positive")
        print(fp_df)
        print("\nFalse negative")
        print(fn_df)
        print("\nambiguous to classify")
        print(ambiguous_df)
        print("""
        false positive : %.3f %%
        false negative : %.3f %%
        TPR            : %.3f %%
        Precision      : %.3f %%
        specificity    : %.3f %%
        accuracy       : %.3f %%
        """ % (false_positive_cnt/total_cnt*100, false_negative_cnt/total_cnt*100, tp_cnt/(tp_cnt+fn_cnt)*100, tp_cnt/(tp_cnt+fp_cnt)*100, tn_cnt/(tn_cnt+fn_cnt)*100, (total_cnt-false_negative_cnt-false_positive_cnt)/total_cnt*100))

    def _modelEvaluationWithSVM(self, result_df) :
        false_positive_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        false_negative_df = pd.DataFrame(index=result_df.index, columns=result_df.columns)
        ambiguous_df = result_df.loc[(-0.1 < result_df["pred_0"]) & (result_df["pred_0"] < 0.1),:]
        false_negative_cnt = 0
        false_positive_cnt = 0
        total_cnt = len(result_df.index)
        tp_df =result_df[(result_df["label_0"] == 1) & (result_df["pred_0"] > 0)]
        fn_df =result_df[(result_df["label_0"] == 1) & (result_df["pred_0"] < 0)]
        tn_df =result_df[(result_df["label_0"] == -1) & (result_df["pred_0"] < 0)]
        fp_df =result_df[(result_df["label_0"] == -1) & (result_df["pred_0"] > 0)]
        tp_cnt = len(tp_df)
        fn_cnt = len(fn_df)
        tn_cnt = len(tn_df)
        fp_cnt = len(fp_df)
        print("\nFalse positive")
        print(fp_df)
        print("\nFalse negative")
        print(fn_df)
        print("\nambiguous to classify")
        print(ambiguous_df)
        print("""
        false positive : %.3f %%
        false negative : %.3f %%
        TPR            : %.3f %%
        Precision      : %.3f %%
        specificity    : %.3f %%
        accuracy       : %.3f %%
        """ % (false_positive_cnt/total_cnt*100, false_negative_cnt/total_cnt*100, tp_cnt/(tp_cnt+fn_cnt)*100, tp_cnt/(tp_cnt+fp_cnt)*100, tn_cnt/(tn_cnt+fn_cnt)*100, (total_cnt-false_negative_cnt-false_positive_cnt)/total_cnt*100))

    def getResult(self) :
        pass
        testPredict = []
        with tf.Session() as sess :
            testPredict = sess.run(self.model.Y_pred_op, feed_dict={self.model.X: self.inputData.test_x, self.model.keep_prob: 1.0})
        return testPredict

    def getResult(self, inputList) :
        pass
        testPredict = []
        with tf.Session() as sess :
            testPredict = sess.run(self.model.Y_pred_op, feed_dict={self.model.X: inputList, self.model.keep_prob: 1.0})
        return testPredict

    def _batchTrainer(self, sess, train_op, batch_divider, trainX, trainY, x_placeholder, y_placeholder, keep_prob, output_keep_prob, input_keep_prob) :
        batch_size = len(trainX)//batch_divider+1
        x_batch = []
        y_batch = []
        i = 0

        while (i < len(trainX)) :
            x_batch.append(trainX[i])
            y_batch.append(trainY[i])
            if ((i+1) % batch_size == 0 or i == len(trainX) - 1) :
                _ = sess.run(train_op, feed_dict={x_placeholder: x_batch, y_placeholder: y_batch, keep_prob: input_keep_prob})
                x_batch = []
                y_batch = []
            i += 1

    def getResultAsDf(self,\
            result=None,\
            testY=None) :

        if result == None :
            result=self.result
            testY=self.inputData.test_y
            testSetIndex=self.inputData.testSetIndex

        df = pd.DataFrame(index=testSetIndex)
        for i in range(0, len(result[0])) :
            df["pred_"+str(i)] = [entry[i] for entry in result]
        for i in range(0, len(testY[0])) :
            df["label_"+str(i)] = [entry[i] for entry in testY]
        return df

def main() :
    global model_type
    nn = NeuralNetwork(model=model_type)
    nn.doTraining()
    nn.getResultAsDf().to_csv(nn.config.learning.resultPath)

main()
